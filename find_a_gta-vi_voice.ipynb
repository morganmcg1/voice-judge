{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align a Voice LLM Judge\n",
    "\n",
    "In this notebook we'll learn how to align a multi-modal LLM judge that can take in audio and guide it towards our preference for what a character from the GTA-VI should sound like.\n",
    "\n",
    "### Who Validates the Validators\n",
    "This human-LLM collaboration is inspired by the [Who Validates the Validators paper](https://arxiv.org/pdf/2404.12272), which focussed on the minimum about of labelling from a human to get the maximum out of aligning a LLM.\n",
    "\n",
    "The repo for this notebook can be found here: https://github.com/morganmcg1/voice-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out if you already have the repo\n",
    "!git clone https://github.com/morganmcg1/voice-judge.git && cp -r voice-judge/* .\n",
    "!uv pip install -r pyproject.toml -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "import weave\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from audio_utils import AudioRanker, wave_read_to_wav_bytes\n",
    "from judge import (\n",
    "    JudgeRanking,\n",
    "    run_speech_llm,\n",
    "    update_pairwise_comparison_history,\n",
    "    # update_pairwise_comparison_history_from_feedback\n",
    ")\n",
    "from preference_learner import PreferenceLearner, USER_CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "### Weave login\n",
    "You'll need a free Weights & Biases account to log in to Weave and run this notebook, sign up here: www.wandb.ai/site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave_client = weave.init(\"voice-judge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio_test:v1\"\n",
    "\n",
    "N_SAMPLES_TO_RANK = 16\n",
    "N_SAMPLES_TO_EVAL = 3\n",
    "\n",
    "PREFERENCE_LEARNER_MODEL =  \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\"\n",
    "JUDGE_MODEL =  \"gemini-2.0-flash\" # \"gemini-2.5-pro-preview-05-06\"\n",
    "ANALYZER_MODEL = \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading speech samples from Weave...\")\n",
    "ds_ref = weave.ref(SPEECH_AUDIO_DATASET_URI).get()\n",
    "speech_samples = list(ds_ref.rows)\n",
    "print(f\"{len(speech_samples)} speech samples downloaded from Weave\")\n",
    "\n",
    "samples_to_rank = {}\n",
    "for i in range(N_SAMPLES_TO_RANK):\n",
    "    samples_to_rank[speech_samples[i][\"voice_instructions_id\"]] = {\n",
    "        \"audio\": speech_samples[i][\"audio\"],\n",
    "        \"audio_bytes\": wave_read_to_wav_bytes(speech_samples[i][\"audio\"]),\n",
    "        \"instructions\": speech_samples[i][\"voice_instructions\"],\n",
    "        \"short_hash\": None,\n",
    "        \"pairwise_comparison_history\": {},\n",
    "    }\n",
    "\n",
    "ids_to_rank = list(samples_to_rank.keys())\n",
    "\n",
    "samples_to_eval = {}\n",
    "for sample in speech_samples[-N_SAMPLES_TO_EVAL:]:\n",
    "    samples_to_eval[sample[\"voice_instructions_id\"]] = {\n",
    "        \"audio\": sample[\"audio\"],\n",
    "        \"audio_bytes\": wave_read_to_wav_bytes(sample[\"audio\"]),\n",
    "        \"instructions\": sample[\"voice_instructions\"],\n",
    "    }\n",
    "ids_to_eval = list(samples_to_eval.keys())\n",
    "\n",
    "full_eval = {}\n",
    "for sample in speech_samples:\n",
    "    full_eval[sample[\"voice_instructions_id\"]] = {\n",
    "        \"audio\": sample[\"audio\"],\n",
    "        \"audio_bytes\": wave_read_to_wav_bytes(sample[\"audio\"]),\n",
    "        \"instructions\": sample[\"voice_instructions\"],\n",
    "    }\n",
    "full_ids_to_eval = list(full_eval.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1 - Rank and Update Judge\n",
    "## Pick first samples to rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our samples for ranking\n",
    "@weave.op\n",
    "def rank_audio(sample_1: dict, sample_2: dict) -> None:\n",
    "    pass\n",
    "\n",
    "sample_1_id = ids_to_rank[0]\n",
    "sample_2_id = ids_to_rank[1]\n",
    "\n",
    "sample_1 = {\"id\": sample_1_id, **samples_to_rank[sample_1_id]}\n",
    "sample_2 = {\"id\": sample_2_id, **samples_to_rank[sample_2_id]}\n",
    "\n",
    "_, target_call = rank_audio.call(sample_1, sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Ranker\n",
    "ranker = AudioRanker(\n",
    "    [\n",
    "        {\n",
    "            \"id\": sample_1_id,\n",
    "            \"audio\": samples_to_rank[sample_1_id][\"audio\"],\n",
    "            \"original_input_order\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"id\": sample_2_id,\n",
    "            # \"audio\": samples_to_rank[sample_2_id][\"audio_bytes\"],\n",
    "            \"audio\": samples_to_rank[sample_2_id][\"audio\"],\n",
    "            \"original_input_order\": 2,\n",
    "        },\n",
    "    ],\n",
    "    weave_client=weave_client,\n",
    "    target_call=target_call,\n",
    "    image_path=\"assets/gta-vi_guy_in_front_of_truck_cropped.jpg\",\n",
    "    mode=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ranker\n",
    "First we'll run our ranker UI and rank the 2 voice samples selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = ranker.create_gradio_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download feedback from Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "updated_target_call = weave_client.get_call(target_call.id)\n",
    "updated_target_call.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert updated_target_call.feedback is not None, \"No feedback found for target call\"\n",
    "for f in updated_target_call.feedback.feedbacks.items:\n",
    "    if \"output\" in f.payload:\n",
    "        ranking = f.payload[\"output\"]\n",
    "        print(\"Rankings retrieved from Weave\")\n",
    "        break\n",
    "\n",
    "rank1 = samples_to_rank[ranking[\"preferred_sample_id\"]]\n",
    "\n",
    "if ranking[\"preferred_sample_id\"] == sample_1_id:\n",
    "    rank2 = samples_to_rank[sample_2_id]\n",
    "    rank2_id = sample_2_id\n",
    "else:\n",
    "    rank2 = samples_to_rank[sample_1_id]\n",
    "    rank2_id = sample_1_id\n",
    "\n",
    "res = [\n",
    "    {\n",
    "        \"rank\" : 1,\n",
    "        \"id\" : ranking[\"preferred_sample_id\"],\n",
    "        \"original_input_order\" : 1 if ranking[\"preferred_sample_id\"] == sample_1_id else 2\n",
    "    },\n",
    "    {\n",
    "        \"rank\" : 2,\n",
    "        \"id\" : rank2_id,\n",
    "        \"original_input_order\" : 1 if rank2_id== sample_1_id else 2\n",
    "    }\n",
    "]\n",
    "\n",
    "final_rankings = {\"rankings\": res, \n",
    "                  \"completed_at\": ranking[\"ranking_timestamp\"],\n",
    "                  \"preferred_id\": ranking[\"preferred_sample_id\"],\n",
    "                  \"rejected_id\": rank2_id\n",
    "                  }\n",
    "final_rankings\n",
    "\n",
    "samples_to_rank = update_pairwise_comparison_history(samples_to_rank, final_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1_id, samples_to_rank[sample_1_id][\"pairwise_comparison_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlysze Preferences\n",
    "\n",
    "Based on the ranking selected, a LLM will now try and identify the differences in the preferred vs the rejected voice sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = PreferenceLearner(model_name=PREFERENCE_LEARNER_MODEL)\n",
    "learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await learner.update(final_rankings, samples_to_rank)\n",
    "ranking_patterns = learner.patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Judge and The Analyzer\n",
    "Below is a seed judge prompt as well as some other prompts for the analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic judge prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_JUDGE_SYSTEM_INSTRUCTION = f\"\"\"Assess the generated voices provided. The task is to assess the appropriateness \\\n",
    "of the audio voice sample for the given task. If multiple voice samples are provided, rank them in order of preference.\n",
    "\n",
    "Here is some context about the task:\n",
    "\n",
    "{USER_CONTEXT_PROMPT}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed_judge_prompt = \"\"\"Based on the following criteria, the task is to assess the appropriateness \\\n",
    "of the audio voice sample for a video game character in his late 50's. He should sound like a man in his late 50's \\\n",
    "and be a little on the wild side.\n",
    "\n",
    "## Voice characteristics to consider\n",
    "Consider the following aspects of the voice:\n",
    "- style\n",
    "- tone\n",
    "- accent\n",
    "- speed\n",
    "- volume\n",
    "- pitch\n",
    "- intonation\n",
    "and more\n",
    "\"\"\"\n",
    "\n",
    "judge_prompt_postfix = \"\"\"## Assessment\n",
    "- If a single voice sample is provided, assess it according to the above criteria and return a bool of whether \\\n",
    "it is appropriate.\n",
    "- If multiple voice samples are provided, rank them in order of preference according to the above criteria.\n",
    "\n",
    "## Voice samples\n",
    "Below are the voice samples to assess:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzer prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_SYSTEM_INSTRUCTION = \"\"\"The task is to help align a LLLM Judge towards user preferences. This will \\\n",
    "be acheived by analysing the current prompt as well as new preference data derived from the user, followed by \\\n",
    "making recommendations for how to update the prompt to align more with the user's preferences.\"\"\"\n",
    "\n",
    "judge_prompt_analyser_prompt = \"\"\"The task is to optimze a given LLM judge prompt to align more with a human \\\n",
    "rater's preferences.\n",
    "\n",
    "## Current prompt\n",
    "Below is the current judge prompt: \n",
    "\n",
    "<current_judge_prompt>\n",
    "{current_judge_prompt}\n",
    "</current_judge_prompt>\n",
    "\n",
    "## Rater preferences\n",
    "Below are patterns that have been observed from a human rater's preferences when doing pairwise comparisons of \\\n",
    "voice samples.\n",
    "\n",
    "<rater_preferences>\n",
    "{ranking_patterns}\n",
    "</rater_preferences>\n",
    "\n",
    "## Analysis\n",
    "Critially analyse the <current_judge_prompt> and <rater_preferences> and make recommendations for whether or not \\\n",
    "the <current_judge_prompt> needs to be updated. You are not required to make edits to the <current_judge_prompt> \\\n",
    "if you fell it is already aligned with the <rater_preferences>.\n",
    "\n",
    "## Output\n",
    "Based on your analysis, please output a full updated judge prompt below. Do not include any placeholder text \\\n",
    "for the input variables. Just focus on describing what makes a good and bad voice sample purely \\\n",
    "based on the <rater_preferences>. Do not include mentions of the outputs needed such as scores or rankings. \\\n",
    "Just describe what makes a good and bad voice sample purely based on the <rater_preferences>.\n",
    "\n",
    "Output your new judge prompt using markdown formatting.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class JudgePromptAnalysis(BaseModel):\n",
    "    reasoning: str = Field(description=\"A detailed explanation of your analysis of the <current_judge_prompt>, \\\n",
    "<rater_preferences> and what edits, if any, are needed to align the <current_judge_prompt> with the rater's preferences.\")\n",
    "    updated_judge_prompt: str = Field(description=\"The updated judge prompt based on the analysis, focussed just on the \\\n",
    "characteristics of a good and bad voice sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Analyzer to update the Judge\n",
    "\n",
    "With these preferences, we'll update a basic \"seed\" LLM Judge prompt to try and make it less generic and more aligned with our personal preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the seed judge prompt plus preference analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def run_judge_prompt_analyser(judge_prompt_analyser_prompt: str, analyzer_model: str) -> JudgePromptAnalysis:\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=ANALYZER_SYSTEM_INSTRUCTION,\n",
    "        prompt=judge_prompt_analyser_prompt,\n",
    "        model_name=analyzer_model,\n",
    "        temperature=1.0,\n",
    "        response_model=JudgePromptAnalysis,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_patterns(patterns):\n",
    "    strong_str = \"\"\n",
    "    for pat in patterns.get('strong', []):\n",
    "        strong_str += f\"  - {pat}\\n\"\n",
    "    emerging_str = \"\"\n",
    "    for pat in patterns.get('emerging', []):\n",
    "        emerging_str += f\"  - {pat}\\n\"\n",
    "    return f\"Strong patterns: {strong_str}\\n\\nEmerging patterns:\\n{emerging_str}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_patterns(ranking_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_str =format_patterns(ranking_patterns)\n",
    "judge_prompt_analyser_prompt = judge_prompt_analyser_prompt.format(\n",
    "    current_judge_prompt=seed_judge_prompt, \n",
    "    ranking_patterns=patterns_str\n",
    ")\n",
    "\n",
    "analyzer_result = await run_judge_prompt_analyser(\n",
    "    judge_prompt_analyser_prompt=judge_prompt_analyser_prompt,\n",
    "    analyzer_model=ANALYZER_MODEL\n",
    ")\n",
    "\n",
    "print(\"**Reasoning:**\")\n",
    "pprint(analyzer_result.reasoning)\n",
    "print()\n",
    "pprint(\"**Updated Judge Prompt:**\")\n",
    "pprint(analyzer_result.updated_judge_prompt)\n",
    "\n",
    "round_1_judge_prompt = analyzer_result.updated_judge_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the updated Judge with the Round 1 judge prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def run_speech_judge(new_judge_prompt, speech_samples_to_eval, judge_model):\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=BASIC_JUDGE_SYSTEM_INSTRUCTION,\n",
    "        prompt=new_judge_prompt,\n",
    "        model_name=judge_model,\n",
    "        temperature=0.1,\n",
    "        response_model=JudgeRanking,\n",
    "        audio_data=[\n",
    "            speech_samples_to_eval[0][\"audio_bytes\"],\n",
    "            speech_samples_to_eval[1][\"audio_bytes\"],\n",
    "            speech_samples_to_eval[2][\"audio_bytes\"],\n",
    "        ],\n",
    "        initial_audio_parts_prompt=\"\\n\\nvoice_1:\\n\",\n",
    "        audio_parts_prompt_divider=\"\\n\\nvoice_{input_order}:\\n\",\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_judge_prompt = round_1_judge_prompt + judge_prompt_postfix\n",
    "speech_samples_to_eval = []\n",
    "for eval_id in ids_to_eval:\n",
    "    speech_samples_to_eval.append(samples_to_eval[eval_id])\n",
    "\n",
    "# Run new judge\n",
    "result = await run_speech_judge(\n",
    "    new_judge_prompt=new_judge_prompt,\n",
    "    speech_samples_to_eval=speech_samples_to_eval,\n",
    "    judge_model=JUDGE_MODEL\n",
    ")\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2 - Rank and Update Judge\n",
    "\n",
    "## Get next samples to rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our samples for ranking\n",
    "sample_1_id = ids_to_rank[2]\n",
    "sample_2_id = ids_to_rank[3]\n",
    "\n",
    "sample_1 = {\"id\": sample_1_id, **samples_to_rank[sample_1_id]}\n",
    "sample_2 = {\"id\": sample_2_id, **samples_to_rank[sample_2_id]}\n",
    "\n",
    "_, target_call = rank_audio.call(sample_1, sample_2)\n",
    "\n",
    "# Init Ranker\n",
    "ranker = AudioRanker(\n",
    "    [\n",
    "        {\n",
    "            \"id\": sample_1_id,\n",
    "            \"audio\": samples_to_rank[sample_1_id][\"audio\"],\n",
    "            \"original_input_order\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"id\": sample_2_id,\n",
    "            # \"audio\": samples_to_rank[sample_2_id][\"audio_bytes\"],\n",
    "            \"audio\": samples_to_rank[sample_2_id][\"audio\"],\n",
    "            \"original_input_order\": 2,\n",
    "        },\n",
    "    ],\n",
    "    weave_client=weave_client,\n",
    "    target_call=target_call,\n",
    "    image_path=\"assets/gta-vi_guy_in_front_of_truck_cropped.jpg\",\n",
    "    mode=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Run the ranker\n",
    "First we'll run our ranker UI and rank the 2 voice samples selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = ranker.create_gradio_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "updated_target_call = weave_client.get_call(target_call.id)\n",
    "updated_target_call.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in updated_target_call.feedback.feedbacks.items:\n",
    "    if \"output\" in f.payload:\n",
    "        ranking = f.payload[\"output\"]\n",
    "        print(\"Rankings retrieved from Weave\")\n",
    "        break\n",
    "\n",
    "rank1 = samples_to_rank[ranking[\"preferred_sample_id\"]]\n",
    "\n",
    "if ranking[\"preferred_sample_id\"] == sample_1_id:\n",
    "    rank2 = samples_to_rank[sample_2_id]\n",
    "    rank2_id = sample_2_id\n",
    "else:\n",
    "    rank2 = samples_to_rank[sample_1_id]\n",
    "    rank2_id = sample_1_id\n",
    "\n",
    "res = [\n",
    "    {\n",
    "        \"rank\" : 1,\n",
    "        \"id\" : ranking[\"preferred_sample_id\"],\n",
    "        \"original_input_order\" : 1 if ranking[\"preferred_sample_id\"] == sample_1_id else 2\n",
    "    },\n",
    "    {\n",
    "        \"rank\" : 2,\n",
    "        \"id\" : rank2_id,\n",
    "        \"original_input_order\" : 1 if rank2_id== sample_1_id else 2\n",
    "    }\n",
    "]\n",
    "\n",
    "final_rankings = {\"rankings\": res, \n",
    "                  \"completed_at\": ranking[\"ranking_timestamp\"],\n",
    "                  \"preferred_id\": ranking[\"preferred_sample_id\"],\n",
    "                  \"rejected_id\": rank2_id\n",
    "                  }\n",
    "final_rankings\n",
    "\n",
    "samples_to_rank = update_pairwise_comparison_history(samples_to_rank, final_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1_id, samples_to_rank[sample_1_id][\"pairwise_comparison_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlysze Preferences\n",
    "\n",
    "Based on the ranking selected, a LLM will now try and identify the differences in the preferred vs the rejected voice sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await learner.update(final_rankings, samples_to_rank)\n",
    "ranking_patterns = learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Judge\n",
    "\n",
    "With these preferences, we'll update a basic \"seed\" LLM Judge prompt to try and make it less generic and more aligned with our personal preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass Round 1 judge prompt plus Round 2 preference analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(round_1_judge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_patterns(ranking_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_analyser_prompt = judge_prompt_analyser_prompt.format(\n",
    "    current_judge_prompt=round_1_judge_prompt,  # <----- Round 1 judge prompt used this time\n",
    "    ranking_patterns=format_patterns(ranking_patterns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_result = await run_judge_prompt_analyser(\n",
    "    judge_prompt_analyser_prompt=judge_prompt_analyser_prompt,\n",
    "    analyzer_model=ANALYZER_MODEL\n",
    ")\n",
    "\n",
    "print(\"**Reasoning:**\")\n",
    "pprint(analyzer_result.reasoning)\n",
    "print()\n",
    "pprint(\"**Updated Judge Prompt:**\")\n",
    "pprint(analyzer_result.updated_judge_prompt)\n",
    "\n",
    "round_2_judge_prompt = analyzer_result.updated_judge_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the updated Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_judge_prompt = round_2_judge_prompt + judge_prompt_postfix\n",
    "speech_samples_to_eval = []\n",
    "for eval_id in ids_to_eval:\n",
    "    speech_samples_to_eval.append(samples_to_eval[eval_id])\n",
    "\n",
    "# Run new judge\n",
    "result = await run_speech_judge(\n",
    "    new_judge_prompt=new_judge_prompt,\n",
    "    speech_samples_to_eval=speech_samples_to_eval,\n",
    "    judge_model=JUDGE_MODEL\n",
    ")\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3 - Rank and Update Judge\n",
    "\n",
    "## Get next samples to rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our samples for ranking\n",
    "sample_1_id = ids_to_rank[4]\n",
    "sample_2_id = ids_to_rank[5]\n",
    "\n",
    "sample_1 = {\"id\": sample_1_id, **samples_to_rank[sample_1_id]}\n",
    "sample_2 = {\"id\": sample_2_id, **samples_to_rank[sample_2_id]}\n",
    "\n",
    "_, target_call = rank_audio.call(sample_1, sample_2)\n",
    "\n",
    "# Init Ranker\n",
    "ranker = AudioRanker(\n",
    "    [\n",
    "        {\n",
    "            \"id\": sample_1_id,\n",
    "            \"audio\": samples_to_rank[sample_1_id][\"audio\"],\n",
    "            \"original_input_order\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"id\": sample_2_id,\n",
    "            # \"audio\": samples_to_rank[sample_2_id][\"audio_bytes\"],\n",
    "            \"audio\": samples_to_rank[sample_2_id][\"audio\"],\n",
    "            \"original_input_order\": 2,\n",
    "        },\n",
    "    ],\n",
    "    weave_client=weave_client,\n",
    "    target_call=target_call,\n",
    "    image_path=\"assets/gta-vi_guy_in_front_of_truck_cropped.jpg\",\n",
    "    mode=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Run the ranker\n",
    "First we'll run our ranker UI and rank the 2 voice samples selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = ranker.create_gradio_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "updated_target_call = weave_client.get_call(target_call.id)\n",
    "updated_target_call.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in updated_target_call.feedback.feedbacks.items:\n",
    "    if \"output\" in f.payload:\n",
    "        ranking = f.payload[\"output\"]\n",
    "        print(\"Rankings retrieved from Weave\")\n",
    "        break\n",
    "\n",
    "rank1 = samples_to_rank[ranking[\"preferred_sample_id\"]]\n",
    "\n",
    "if ranking[\"preferred_sample_id\"] == sample_1_id:\n",
    "    rank2 = samples_to_rank[sample_2_id]\n",
    "    rank2_id = sample_2_id\n",
    "else:\n",
    "    rank2 = samples_to_rank[sample_1_id]\n",
    "    rank2_id = sample_1_id\n",
    "\n",
    "res = [\n",
    "    {\n",
    "        \"rank\" : 1,\n",
    "        \"id\" : ranking[\"preferred_sample_id\"],\n",
    "        \"original_input_order\" : 1 if ranking[\"preferred_sample_id\"] == sample_1_id else 2\n",
    "    },\n",
    "    {\n",
    "        \"rank\" : 2,\n",
    "        \"id\" : rank2_id,\n",
    "        \"original_input_order\" : 1 if rank2_id== sample_1_id else 2\n",
    "    }\n",
    "]\n",
    "\n",
    "final_rankings = {\"rankings\": res, \n",
    "                  \"completed_at\": ranking[\"ranking_timestamp\"],\n",
    "                  \"preferred_id\": ranking[\"preferred_sample_id\"],\n",
    "                  \"rejected_id\": rank2_id\n",
    "                  }\n",
    "final_rankings\n",
    "\n",
    "samples_to_rank = update_pairwise_comparison_history(samples_to_rank, final_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1_id, samples_to_rank[sample_1_id][\"pairwise_comparison_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlysze Preferences\n",
    "\n",
    "Based on the ranking selected, a LLM will now try and identify the differences in the preferred vs the rejected voice sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await learner.update(final_rankings, samples_to_rank)\n",
    "ranking_patterns = learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Judge\n",
    "\n",
    "With these preferences, we'll update a basic \"seed\" LLM Judge prompt to try and make it less generic and more aligned with our personal preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass Round 2 judge prompt plus Round 3 preference analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(round_2_judge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_patterns(ranking_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_analyser_prompt = judge_prompt_analyser_prompt.format(\n",
    "    current_judge_prompt=round_2_judge_prompt,  # <----- Round 2 judge prompt used this time\n",
    "    ranking_patterns=format_patterns(ranking_patterns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_result = await run_judge_prompt_analyser(\n",
    "    judge_prompt_analyser_prompt=judge_prompt_analyser_prompt,\n",
    "    analyzer_model=ANALYZER_MODEL\n",
    ")\n",
    "\n",
    "print(\"**Reasoning:**\")\n",
    "pprint(analyzer_result.reasoning)\n",
    "print()\n",
    "pprint(\"**Updated Judge Prompt:**\")\n",
    "pprint(analyzer_result.updated_judge_prompt)\n",
    "\n",
    "round_3_judge_prompt = analyzer_result.updated_judge_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the updated Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_judge_prompt = round_3_judge_prompt + judge_prompt_postfix\n",
    "speech_samples_to_eval = []\n",
    "for eval_id in ids_to_eval:\n",
    "    speech_samples_to_eval.append(samples_to_eval[eval_id])\n",
    "\n",
    "# Run new judge\n",
    "result = await run_speech_judge(\n",
    "    new_judge_prompt=new_judge_prompt,\n",
    "    speech_samples_to_eval=speech_samples_to_eval,\n",
    "    judge_model=JUDGE_MODEL\n",
    ")\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_judge_prompt = round_3_judge_prompt + judge_prompt_postfix\n",
    "speech_samples_to_eval = []\n",
    "for eval_id in full_ids_to_eval:\n",
    "    speech_samples_to_eval.append(samples_to_eval[eval_id])\n",
    "\n",
    "print(f\"Running new judge on {len(speech_samples_to_eval)} samples\")\n",
    "\n",
    "# Run new judge\n",
    "result = await run_speech_judge(\n",
    "    new_judge_prompt=new_judge_prompt,\n",
    "    speech_samples_to_eval=speech_samples_to_eval,\n",
    "    judge_model=JUDGE_MODEL\n",
    ")\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
