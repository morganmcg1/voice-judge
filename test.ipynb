{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases is Great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/morganmcg1/voice-judge.git && cp -r voice-judge/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r pyproject.toml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "import weave\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from audio_utils import AudioRanker, wave_read_to_wav_bytes\n",
    "from judge import (\n",
    "    JudgeRanking,\n",
    "    run_speech_llm,\n",
    "    update_pairwise_comparison_history,\n",
    "    # update_pairwise_comparison_history_from_feedback\n",
    ")\n",
    "from preference_learner import PreferenceLearner, USER_CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: morgan.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-voice-ai/voice-judge/weave\n"
     ]
    }
   ],
   "source": [
    "weave_client = weave.init(\"wandb-voice-ai/voice-judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading speech samples from Weave...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b1-92c5-7943-a36b-533e40d5a4e8\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b1-e0fc-72d1-a2c5-924020c65cf4\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b4-2177-7bc1-8ad8-9f7af54bccfb\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b6-a97d-7c83-bf52-0e69c3f7fec8\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b7-1463-79e2-804c-0775d5fb49d3\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b9-608f-7bc0-a0a7-46061852d832\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b9-d21c-75e1-ac79-cf68198cb453\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712ba-be2c-7d52-8328-8189485cd398\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712ba-e089-71d0-babd-6bffd0498ccd\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712bb-dd3e-7460-8d03-3f4421cd3ad4\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712bc-8fcd-7ef1-9026-ebcc5ba864a1\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712bd-5e48-7b31-9bcf-828aadb4c418\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712be-e64b-7421-be13-5bcea1f74f39\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712c3-bba4-78a1-8bc4-552e0fd04bad\n"
     ]
    }
   ],
   "source": [
    "# SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio:v1\"\n",
    "SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio_test:v1\"\n",
    "N_SAMPLES_TO_RANK = 3\n",
    "PREFERENCE_LEARNER_MODEL =  \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\"\n",
    "JUDGE_MODEL =  \"gemini-2.0-flash\" # \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "print(\"Downloading speech samples from Weave...\")\n",
    "ds_ref = weave.ref(SPEECH_AUDIO_DATASET_URI).get()\n",
    "speech_samples = list(ds_ref.rows)\n",
    "print(f\"{len(speech_samples)} speech samples downloaded from Weave\")\n",
    "\n",
    "samples_to_rank = {}\n",
    "for i in range(N_SAMPLES_TO_RANK):\n",
    "    samples_to_rank[speech_samples[i][\"voice_instructions_id\"]] = {\n",
    "        \"audio\": speech_samples[i][\"audio\"],\n",
    "        \"audio_bytes\": wave_read_to_wav_bytes(speech_samples[i][\"audio\"]),\n",
    "        \"instructions\": speech_samples[i][\"voice_instructions\"],\n",
    "        \"short_hash\": None,\n",
    "        \"pairwise_comparison_history\": {},\n",
    "    }\n",
    "\n",
    "ids_to_rank = list(samples_to_rank.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our samples for ranking\n",
    "@weave.op\n",
    "def rank_audio(sample_1: dict, sample_2: dict) -> None:\n",
    "    pass\n",
    "\n",
    "sample_1_id = ids_to_rank[0]\n",
    "sample_2_id = ids_to_rank[1]\n",
    "\n",
    "sample_1 = {\"id\": sample_1_id, **samples_to_rank[sample_1_id]}\n",
    "sample_2 = {\"id\": sample_2_id, **samples_to_rank[sample_2_id]}\n",
    "\n",
    "_, target_call = rank_audio.call(sample_1, sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-voice-ai/voice-judge/weave/objects/AudioRanker/versions/ym5a2rKViLvZtIjKh5VMKYzRxpZOmGhGDTLgSi8hDN8\n"
     ]
    }
   ],
   "source": [
    "# Init Ranker\n",
    "\n",
    "ranker = AudioRanker(\n",
    "    [\n",
    "        {\n",
    "            \"id\": sample_1_id,\n",
    "            \"audio\": samples_to_rank[sample_1_id][\"audio\"],\n",
    "            \"original_input_order\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"id\": sample_2_id,\n",
    "            # \"audio\": samples_to_rank[sample_2_id][\"audio_bytes\"],\n",
    "            \"audio\": samples_to_rank[sample_2_id][\"audio\"],\n",
    "            \"original_input_order\": 2,\n",
    "        },\n",
    "    ],\n",
    "    weave_client=weave_client,\n",
    "    target_call=target_call,\n",
    "    image_path=\"assets/gta-vi_guy_in_front_of_truck_cropped.jpg\",\n",
    "    mode=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BinaryVoiceRank applied successfully to weave call. Result: ApplyScorerSuccess(result={'preferred_sample_id': 'weary_georgia_mutter_20250526_2033', 'sample_one_preferred': False, 'sample_two_preferred': True, 'ranking_timestamp': '2025-05-27_18-02'}, score_call=Call(_op_name=<Future at 0x11b180a10 state=running>, trace_id='019712b1-e0fc-72d1-a2c5-9237d50b0985', project_id='wandb-voice-ai/voice-judge', parent_id=None, inputs={'self': ObjectRef(entity='wandb-voice-ai', project='voice-judge', name='BinaryVoiceRank', _digest=<Future at 0x11aee9390 state=pending>, _extra=()), 'output': None}, id='019712b1-e0fc-72d1-a2c5-924020c65cf4', output={'preferred_sample_id': 'weary_georgia_mutter_20250526_2033', 'sample_one_preferred': False, 'sample_two_preferred': True, 'ranking_timestamp': '2025-05-27_18-02'}, exception=None, summary={'status_counts': {<TraceStatus.SUCCESS: 'success'>: 1, <TraceStatus.ERROR: 'error'>: 0}}, _display_name=None, attributes=AttributesDict({'weave': {'python': {'type': 'function'}, 'client_version': '0.51.48', 'source': 'python-sdk', 'sys_version': '3.11.11 (main, Mar 17 2025, 10:36:47) [Clang 16.0.0 (clang-1600.0.26.6)]', 'os_name': 'Darwin', 'os_version': 'Darwin Kernel Version 24.5.0: Tue Apr 22 19:48:46 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8103', 'os_release': '24.5.0'}}), started_at=None, ended_at=datetime.datetime(2025, 5, 27, 17, 2, 16, 829243, tzinfo=datetime.timezone.utc), deleted_at=None, _children=[], _feedback=None))\n"
     ]
    }
   ],
   "source": [
    "interface = ranker.create_gradio_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do ranking\n",
    "# ranker.display_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "updated_target_call = weave_client.get_call(target_call.id)\n",
    "updated_target_call.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings retried from Weave\n"
     ]
    }
   ],
   "source": [
    "for f in updated_target_call.feedback.feedbacks.items:\n",
    "    if \"output\" in f.payload:\n",
    "        ranking = f.payload[\"output\"]\n",
    "        print(\"Rankings retried from Weave\")\n",
    "        break\n",
    "\n",
    "rank1 = samples_to_rank[ranking[\"preferred_sample_id\"]]\n",
    "\n",
    "if ranking[\"preferred_sample_id\"] == sample_1_id:\n",
    "    rank2 = samples_to_rank[sample_2_id]\n",
    "    rank2_id = sample_2_id\n",
    "else:\n",
    "    rank2 = samples_to_rank[sample_1_id]\n",
    "    rank2_id = sample_1_id\n",
    "\n",
    "res = [\n",
    "    {\n",
    "        \"rank\" : 1,\n",
    "        \"id\" : ranking[\"preferred_sample_id\"],\n",
    "        \"original_input_order\" : 1 if ranking[\"preferred_sample_id\"] == sample_1_id else 2\n",
    "    },\n",
    "    {\n",
    "        \"rank\" : 2,\n",
    "        \"id\" : rank2_id,\n",
    "        \"original_input_order\" : 1 if rank2_id== sample_1_id else 2\n",
    "    }\n",
    "]\n",
    "\n",
    "final_rankings = {\"rankings\": res, \n",
    "                  \"completed_at\": ranking[\"ranking_timestamp\"],\n",
    "                  \"preferred_id\": ranking[\"preferred_sample_id\"],\n",
    "                  \"rejected_id\": rank2_id\n",
    "                  }\n",
    "final_rankings\n",
    "\n",
    "samples_to_rank = update_pairwise_comparison_history(samples_to_rank, final_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('surprised_french_veteran_20250526_2033',\n",
       " {'2025-05-27_18-02': {'competitor_id': 'weary_georgia_mutter_20250526_2033',\n",
       "   'sample_rank_in_this_pair': 2}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1_id, samples_to_rank[sample_1_id][\"pairwise_comparison_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strong': [], 'emerging': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PreferenceLearner()\n",
    "learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating comparisons...\n",
      "Running pattern update...\n",
      "\n",
      "\n",
      "Pattern update result:\n",
      "('reasoning: In the provided example, the preferred voice exhibits a slight '\n",
      " 'increase in pitch and more varied intonation, making it sound more '\n",
      " 'expressive and engaging. The rejected voice sounds flatter and less '\n",
      " 'enthusiastic. This suggests that higher pitch and varied intonation may be '\n",
      " 'preferred. Since there are no existing strong or emerging patterns, these '\n",
      " 'are considered new.')\n",
      "'strong: []'\n",
      "\"emerging: ['Higher pitch is preferred.', 'Varied intonation is preferred.']\"\n"
     ]
    }
   ],
   "source": [
    "await learner.update(final_rankings, samples_to_rank)\n",
    "ranking_patterns = learner.patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_JUDGE_SYSTEM_INSTRUCTION = f\"\"\"Assess the generated voices provided. The task is to assess the appropriateness \\\n",
    "of the audio voice sample for the given task. If multiple voice samples are provided, rank them in order of preference.\n",
    "\n",
    "Here is some context about the task:\n",
    "\n",
    "{USER_CONTEXT_PROMPT}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed_judge_prompt = f\"\"\"Based on the following criteria, the task is to assess the appropriateness \\\n",
    "of the audio voice sample for a video game character in his late 50's. He should sound like a man in his late 50's \\\n",
    "and be a little on the wild side.\n",
    "\n",
    "## Voice characteristics to consider\n",
    "Consider the following aspects of the voice:\n",
    "- style\n",
    "- tone\n",
    "- accent\n",
    "- speed\n",
    "- volume\n",
    "- pitch\n",
    "- intonation\n",
    "and more\n",
    "\"\"\"\n",
    "\n",
    "judge_prompt_postfix = f\"\"\"## Assessment\n",
    "- If a single voice sample is provided, assess it according to the above criteria and return a bool of whether \\\n",
    "it is appropriate.\n",
    "- If multiple voice samples are provided, rank them in order of preference according to the above criteria.\n",
    "\n",
    "## Voice samples\n",
    "Below are the voice samples to assess:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ---------Analyzer Prompts ---------\n",
    "\n",
    "ANALYZER_SYSTEM_INSTRUCTION = f\"\"\"The task is to help align a LLLM Judge towards user preferences. This will \\\n",
    "be acheived by analysing the current prompt as well as new preference data derived from the user, followed by \\\n",
    "making recommendations for how to update the prompt to align more with the user's preferences.\"\"\"\n",
    "\n",
    "judge_prompt_analyser_prompt = \"\"\"The task is to optimze a given LLM judge prompt to align more with a human \\\n",
    "rater's preferences.\n",
    "\n",
    "## Current prompt\n",
    "Below is the current judge prompt: \n",
    "\n",
    "<current_judge_prompt>\n",
    "{current_judge_prompt}\n",
    "</current_judge_prompt>\n",
    "\n",
    "## Rater preferences\n",
    "Below are patterns that have been observed from a human rater's preferences when doing pairwise comparisons of \\\n",
    "voice samples.\n",
    "\n",
    "<rater_preferences>\n",
    "{ranking_patterns}\n",
    "</rater_preferences>\n",
    "\n",
    "## Analysis\n",
    "Critially analyse the <current_judge_prompt> and <rater_preferences> and make recommendations for whether or not \\\n",
    "the <current_judge_prompt> needs to be updated. You are not required to make edits to the <current_judge_prompt> \\\n",
    "if you fell it is already aligned with the <rater_preferences>.\n",
    "\n",
    "## Output\n",
    "Based on your analysis, please output a full updated judge prompt below. Do not include any placeholder text \\\n",
    "for the input variables. Just focus on describing what makes a good and bad voice sample purely \\\n",
    "based on the <rater_preferences>. Do not include mentions of the outputs needed such as scores or rankings. \\\n",
    "Just describe what makes a good and bad voice sample purely based on the <rater_preferences>.\n",
    "\n",
    "Output your new judge prompt using markdown formatting.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class JudgePromptAnalysis(BaseModel):\n",
    "    reasoning: str = Field(description=\"A detailed explanation of your analysis of the <current_judge_prompt>, \\\n",
    "<rater_preferences> and what edits, if any, are needed to align the <current_judge_prompt> with the rater's preferences.\")\n",
    "    updated_judge_prompt: str = Field(description=\"The updated judge prompt based on the analysis, focussed just on the \\\n",
    "characteristics of a good and bad voice sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the seed judge prompt plus preference analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def run_judge_prompt_analyser(judge_prompt_analyser_prompt: str, analyzer_model: str) -> JudgePromptAnalysis:\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=ANALYZER_SYSTEM_INSTRUCTION,\n",
    "        prompt=judge_prompt_analyser_prompt,\n",
    "        model_name=analyzer_model,\n",
    "        temperature=1.0,\n",
    "        response_model=JudgePromptAnalysis,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Error getting code deps for <function run_judge_prompt_analyser at 0x11b7d3600>: closing parenthesis '}' does not match opening parenthesis '(' (<unknown>, line 419)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Reasoning:**\n",
      "('The current prompt provides general guidelines for assessing voice samples '\n",
      " \"but doesn't explicitly emphasize the rater's preferences for higher pitch \"\n",
      " 'and varied intonation. The updated prompt will incorporate these preferences '\n",
      " \"to better align with the rater's taste.\")\n",
      "\n",
      "'**Updated Judge Prompt:**'\n",
      "('A good voice sample for the video game character should exhibit the '\n",
      " 'following characteristics:\\n'\n",
      " '\\n'\n",
      " '*   The voice should sound like a man in his late 50s, embodying a slightly '\n",
      " 'wild persona.\\n'\n",
      " '*   The pitch should be relatively high.\\n'\n",
      " '*   The intonation should be varied and expressive.\\n'\n",
      " '\\n'\n",
      " 'A bad voice sample would sound too young or too old and lack the desired '\n",
      " 'wild quality. Crucially, it would have a lower pitch and/or monotonous '\n",
      " 'intonation.')\n"
     ]
    }
   ],
   "source": [
    "judge_prompt_analyser_prompt = judge_prompt_analyser_prompt.format(current_judge_prompt=seed_judge_prompt, ranking_patterns=ranking_patterns)\n",
    "ANALYZER_MODEL = \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "analyzer_result = await run_judge_prompt_analyser(\n",
    "    judge_prompt_analyser_prompt=judge_prompt_analyser_prompt,\n",
    "    analyzer_model=ANALYZER_MODEL\n",
    ")\n",
    "\n",
    "print(\"**Reasoning:**\")\n",
    "pprint(analyzer_result.reasoning)\n",
    "print()\n",
    "pprint(\"**Updated Judge Prompt:**\")\n",
    "pprint(analyzer_result.updated_judge_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the updated Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def run_speech_judge(new_judge_prompt, speech_samples_to_eval, judge_model):\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=BASIC_JUDGE_SYSTEM_INSTRUCTION,\n",
    "        prompt=new_judge_prompt,\n",
    "        model_name=judge_model,\n",
    "        temperature=0.1,\n",
    "        response_model=JudgeRanking,\n",
    "        audio_data=[\n",
    "            speech_samples_to_eval[0][\"audio_bytes\"],\n",
    "            speech_samples_to_eval[1][\"audio_bytes\"],\n",
    "            speech_samples_to_eval[2][\"audio_bytes\"],\n",
    "        ],\n",
    "        initial_audio_parts_prompt=\"\\n\\nvoice_1:\\n\",\n",
    "        audio_parts_prompt_divider=\"\\n\\nvoice_{input_order}:\\n\",\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeaveDict({'voice_instructions_id': 'furious_fast_frenchman_20250526_2033', 'voice_instructions': 'Voice/Affect: Late-50s French male, slightly gravelly from years of smoking; clear Parisian accent on English words.\\nTone: Explosively angry, incredulous, borderline shouting.\\nPacing/Delivery: Super-fast, breathless, almost tripping over words; give only a razor-thin pause after the question mark before the accusation.\\nVolume: Loud, projecting frustration.\\nEmotion: White-hot irritation with a hint of long-simmering fatigue.\\nPronunciation: Spell out \"G-T-A six\" as separate English letters \"G\", \"T\", \"A\" then the word \"six\"; punch the word \"now\" and the phrase \"by hand\".\\nLine to speak (exact): \"What are you doing up there? GTA 6 was supposed to be out by now — you coding the whole thing by hand or what?!\"', 'audio_file_path': 'generated_audio/audio_furious_fast_frenchman_20250526_2033.wav', 'tts_model_name': 'gpt-4o-mini-tts', 'voice': 'onyx', 'audio': <wave.Wave_read object at 0x11a847690>})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_samples_to_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m speech_samples_to_eval = speech_samples[-\u001b[32m3\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Run new judge\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m run_speech_judge(\n\u001b[32m      6\u001b[39m     new_judge_prompt=new_judge_prompt,\n\u001b[32m      7\u001b[39m     speech_samples_to_eval=speech_samples_to_eval,\n\u001b[32m      8\u001b[39m     judge_model=JUDGE_MODEL\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m pprint(result.thinking)\n\u001b[32m     11\u001b[39m pprint(result.ranking)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/voice-judge/.venv/lib/python3.11/site-packages/weave/trace/op.py:1224\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1222\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1223\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:  \u001b[38;5;66;03m# pyright: ignore[reportRedeclaration]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m     res, _ = \u001b[38;5;28;01mawait\u001b[39;00m _call_async_func(\n\u001b[32m   1225\u001b[39m         cast(Op[P, R], wrapper), *args, __should_raise=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs\n\u001b[32m   1226\u001b[39m     )\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/voice-judge/.venv/lib/python3.11/site-packages/weave/trace/op.py:636\u001b[39m, in \u001b[36m_call_async_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    633\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     res = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    638\u001b[39m     finish(exception=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mrun_speech_judge\u001b[39m\u001b[34m(new_judge_prompt, speech_samples_to_eval, judge_model)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@weave\u001b[39m.op\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_speech_judge\u001b[39m(new_judge_prompt, speech_samples_to_eval, judge_model):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Example with file path\u001b[39;00m\n\u001b[32m      4\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m run_speech_llm(\n\u001b[32m      5\u001b[39m         system_instruction=BASIC_JUDGE_SYSTEM_INSTRUCTION,\n\u001b[32m      6\u001b[39m         prompt=new_judge_prompt,\n\u001b[32m      7\u001b[39m         model_name=judge_model,\n\u001b[32m      8\u001b[39m         temperature=\u001b[32m0.1\u001b[39m,\n\u001b[32m      9\u001b[39m         response_model=JudgeRanking,\n\u001b[32m     10\u001b[39m         audio_data=[\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m             \u001b[43mspeech_samples_to_eval\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33maudio_bytes\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m             speech_samples_to_eval[\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maudio_bytes\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m             speech_samples_to_eval[\u001b[32m2\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33maudio_bytes\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     14\u001b[39m         ],\n\u001b[32m     15\u001b[39m         initial_audio_parts_prompt=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mvoice_1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         audio_parts_prompt_divider=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mvoice_\u001b[39m\u001b[38;5;132;01m{input_order}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/voice-judge/.venv/lib/python3.11/site-packages/weave/trace/vals.py:689\u001b[39m, in \u001b[36mWeaveDict.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m    688\u001b[39m     new_ref = \u001b[38;5;28mself\u001b[39m.ref.with_key(key) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ref \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     v = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m make_trace_obj(v, new_ref, \u001b[38;5;28mself\u001b[39m.server, \u001b[38;5;28mself\u001b[39m.root)\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "new_judge_prompt = analyzer_result.updated_judge_prompt + judge_prompt_postfix\n",
    "speech_samples_to_eval = speech_samples[-3]\n",
    "\n",
    "# Run new judge\n",
    "result = await run_speech_judge(\n",
    "    new_judge_prompt=new_judge_prompt,\n",
    "    speech_samples_to_eval=speech_samples_to_eval,\n",
    "    judge_model=JUDGE_MODEL\n",
    ")\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
