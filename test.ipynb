{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases is Great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/morganmcg1/voice-judge.git && cp -r voice-judge/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r pyproject.toml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "import weave\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from audio_utils import AudioRanker, wave_read_to_wav_bytes\n",
    "from judge import (\n",
    "    JudgeRanking,\n",
    "    run_speech_llm,\n",
    "    # update_pairwise_comparison_history,\n",
    "    update_pairwise_comparison_history_from_feedback\n",
    ")\n",
    "from preference_learner import PreferenceLearner, USER_CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: morgan.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-voice-ai/voice-judge/weave\n"
     ]
    }
   ],
   "source": [
    "weave_client = weave.init(\"wandb-voice-ai/voice-judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading speech samples from Weave...\n"
     ]
    }
   ],
   "source": [
    "# SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio:v1\"\n",
    "SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio_test\"\n",
    "N_SAMPLES_TO_RANK = 3\n",
    "PREFERENCE_LEARNER_MODEL =  \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\"\n",
    "JUDGE_MODEL =  \"gemini-2.0-flash\" # \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "print(\"Downloading speech samples from Weave...\")\n",
    "ds_ref = weave.ref(SPEECH_AUDIO_DATASET_URI).get()\n",
    "speech_samples = list(ds_ref.rows)\n",
    "\n",
    "samples_to_rank = {}\n",
    "for i in range(N_SAMPLES_TO_RANK):\n",
    "    samples_to_rank[speech_samples[i][\"voice_instructions_id\"]] = {\n",
    "        \"audio\": speech_samples[i][\"audio\"],\n",
    "        \"audio_bytes\": wave_read_to_wav_bytes(speech_samples[i][\"audio\"]),\n",
    "        \"instructions\": speech_samples[i][\"voice_instructions\"],\n",
    "        \"short_hash\": None,\n",
    "        \"pairwise_comparison_history\": {},\n",
    "    }\n",
    "\n",
    "ids_to_rank = list(samples_to_rank.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our samples for ranking\n",
    "@weave.op\n",
    "def rank_audio(sample_1: dict, sample_2: dict) -> None:\n",
    "    pass\n",
    "\n",
    "sample_1_id = ids_to_rank[0]\n",
    "sample_2_id = ids_to_rank[1]\n",
    "\n",
    "sample_1 = {\"id\": sample_1_id, **samples_to_rank[sample_1_id]}\n",
    "sample_2 = {\"id\": sample_2_id, **samples_to_rank[sample_2_id]}\n",
    "\n",
    "_, target_call = rank_audio.call(sample_1, sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb-voice-ai/voice-judge/r/call/0197128e-d24f-72f0-bd31-833e348ac9ed\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb-voice-ai/voice-judge/r/call/0197128f-02ea-7823-90a8-ddd1d9dead6b\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712ad-f95a-7111-a3ea-4dce60d6960f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üì¶ Published to https://wandb.ai/wandb-voice-ai/voice-judge/weave/objects/AudioRanker/versions/ym5a2rKViLvZtIjKh5VMKYzRxpZOmGhGDTLgSi8hDN8\n"
     ]
    }
   ],
   "source": [
    "# Init Ranker\n",
    "\n",
    "ranker = AudioRanker(\n",
    "    [\n",
    "        {\n",
    "            \"id\": sample_1_id,\n",
    "            \"audio\": samples_to_rank[sample_1_id][\"audio\"],\n",
    "            \"original_input_order\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"id\": sample_2_id,\n",
    "            # \"audio\": samples_to_rank[sample_2_id][\"audio_bytes\"],\n",
    "            \"audio\": samples_to_rank[sample_2_id][\"audio\"],\n",
    "            \"original_input_order\": 2,\n",
    "        },\n",
    "    ],\n",
    "    weave_client=weave_client,\n",
    "    target_call=target_call,\n",
    "    image_path=\"assets/gta-vi_guy_in_front_of_truck_cropped.jpg\",\n",
    "    mode=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BinaryVoiceRank applied successfully to weave call. Result: ApplyScorerSuccess(result={'preferred_sample_id': 'weary_georgia_mutter_20250526_2033', 'sample_one_preferred': False, 'sample_two_preferred': True, 'ranking_timestamp': '2025-05-27_17-24'}, score_call=Call(_op_name=<Future at 0x12a9a46d0 state=running>, trace_id='0197128f-02ea-7823-90a8-ddce48ea87c2', project_id='wandb-voice-ai/voice-judge', parent_id=None, inputs={'self': ObjectRef(entity='wandb-voice-ai', project='voice-judge', name='BinaryVoiceRank', _digest=<Future at 0x12a999b10 state=pending>, _extra=()), 'output': None}, id='0197128f-02ea-7823-90a8-ddd1d9dead6b', output={'preferred_sample_id': 'weary_georgia_mutter_20250526_2033', 'sample_one_preferred': False, 'sample_two_preferred': True, 'ranking_timestamp': '2025-05-27_17-24'}, exception=None, summary={'status_counts': {<TraceStatus.SUCCESS: 'success'>: 1, <TraceStatus.ERROR: 'error'>: 0}}, _display_name=None, attributes=AttributesDict({'weave': {'python': {'type': 'function'}, 'client_version': '0.51.48', 'source': 'python-sdk', 'sys_version': '3.11.11 (main, Mar 17 2025, 10:36:47) [Clang 16.0.0 (clang-1600.0.26.6)]', 'os_name': 'Darwin', 'os_version': 'Darwin Kernel Version 24.5.0: Tue Apr 22 19:48:46 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8103', 'os_release': '24.5.0'}}), started_at=None, ended_at=datetime.datetime(2025, 5, 27, 16, 24, 11, 754996, tzinfo=datetime.timezone.utc), deleted_at=None, _children=[], _feedback=None))\n"
     ]
    }
   ],
   "source": [
    "interface = ranker.create_gradio_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do ranking\n",
    "# ranker.display_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "updated_target_call = weave_client.get_call(target_call.id)\n",
    "updated_target_call.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in updated_target_call.feedback.feedbacks.items:\n",
    "    if \"output\" in f.payload:\n",
    "        ranking = f.payload[\"output\"]\n",
    "        samples_to_rank = update_pairwise_comparison_history_from_feedback(samples_to_rank, ranking)\n",
    "        print(\"Updated samples_to_rank\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preferred_sample_id': 'weary_georgia_mutter_20250526_2033',\n",
       " 'sample_one_preferred': False,\n",
       " 'sample_two_preferred': True,\n",
       " 'ranking_timestamp': '2025-05-27_17-24'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rankings': [{'rank': 1,\n",
       "   'id': 'weary_georgia_mutter_20250526_2033',\n",
       "   'original_input_order': 2},\n",
       "  {'rank': 2,\n",
       "   'id': 'surprised_french_veteran_20250526_2033',\n",
       "   'original_input_order': 1}],\n",
       " 'completed_at': '2025-05-27_17-24',\n",
       " 'preferred_id': 'weary_georgia_mutter_20250526_2033',\n",
       " 'rejected_id': 'surprised_french_veteran_20250526_2033'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1 = samples_to_rank[ranking[\"preferred_sample_id\"]]\n",
    "\n",
    "if ranking[\"preferred_sample_id\"] == sample_1_id:\n",
    "    rank2 = samples_to_rank[sample_2_id]\n",
    "    rank2_id = sample_2_id\n",
    "else:\n",
    "    rank2 = samples_to_rank[sample_1_id]\n",
    "    rank2_id = sample_1_id\n",
    "\n",
    "res = [\n",
    "    {\n",
    "        \"rank\" : 1,\n",
    "        \"id\" : ranking[\"preferred_sample_id\"],\n",
    "        \"original_input_order\" : 1 if ranking[\"preferred_sample_id\"] == sample_1_id else 2\n",
    "    },\n",
    "    {\n",
    "        \"rank\" : 2,\n",
    "        \"id\" : rank2_id,\n",
    "        \"original_input_order\" : 1 if rank2_id== sample_1_id else 2\n",
    "    }\n",
    "]\n",
    "\n",
    "final_rankings = {\"rankings\": res, \n",
    "                  \"completed_at\": ranking[\"ranking_timestamp\"],\n",
    "                  \"preferred_id\": ranking[\"preferred_sample_id\"],\n",
    "                  \"rejected_id\": rank2_id\n",
    "                  }\n",
    "final_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strong': [], 'emerging': []}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PreferenceLearner()\n",
    "learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Error getting code deps for <function run_speech_llm at 0x11a883060>: lineno is out of bounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating comparisons...\n",
      "Running pattern update...\n",
      "\n",
      "\n",
      "Pattern update result:\n",
      "('reasoning: The preferred voice in the recent comparison has slightly more '\n",
      " 'emphasized intonation and sounds more conversational than the rejected '\n",
      " 'voice. Thus, emphasis on conversational tone is emerging.')\n",
      "'strong: []'\n",
      "\"emerging: ['More conversational tone preferred']\"\n"
     ]
    }
   ],
   "source": [
    "await learner.update(final_rankings, samples_to_rank)\n",
    "ranking_patterns = learner.patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_JUDGE_SYSTEM_INSTRUCTION = f\"\"\"Assess the generated voices provided. The task is to assess the appropriateness \\\n",
    "of the audio voice sample for the given task. If multiple voice samples are provided, rank them in order of preference.\n",
    "\n",
    "Here is some context about the task:\n",
    "\n",
    "{USER_CONTEXT_PROMPT}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed_judge_prompt = f\"\"\"Based on the following criteria, the task is to assess the appropriateness \\\n",
    "of the audio voice sample for a video game character in his late 50's. He should sound like a man in his late 50's \\\n",
    "and be a little on the wild side.\n",
    "\n",
    "## Voice characteristics to consider\n",
    "Consider the following aspects of the voice:\n",
    "- style\n",
    "- tone\n",
    "- accent\n",
    "- speed\n",
    "- volume\n",
    "- pitch\n",
    "- intonation\n",
    "and more\n",
    "\"\"\"\n",
    "\n",
    "judge_prompt_postfix = f\"\"\"## Assessment\n",
    "- If a single voice sample is provided, assess it according to the above criteria and return a bool of whether \\\n",
    "it is appropriate.\n",
    "- If multiple voice samples are provided, rank them in order of preference according to the above criteria.\n",
    "\"\"\"\n",
    "\n",
    "# ---------Analyzer Prompts ---------\n",
    "\n",
    "ANALYZER_SYSTEM_INSTRUCTION = f\"\"\"The task is to help align a LLLM Judge towards user preferences. This will \\\n",
    "be acheived by analysing the current prompt as well as new preference data derived from the user, followed by \\\n",
    "making recommendations for how to update the prompt to align more with the user's preferences.\"\"\"\n",
    "\n",
    "judge_prompt_analyser_prompt = \"\"\"The task is to optimze a given LLM judge prompt to align more with a human \\\n",
    "rater's preferences.\n",
    "\n",
    "## Current prompt\n",
    "Below is the current judge prompt: \n",
    "\n",
    "<current_judge_prompt>\n",
    "{current_judge_prompt}\n",
    "</current_judge_prompt>\n",
    "\n",
    "## Rater preferences\n",
    "Below are patterns that have been observed from a human rater's preferences when doing pairwise comparisons of \\\n",
    "voice samples.\n",
    "\n",
    "<rater_preferences>\n",
    "{ranking_patterns}\n",
    "</rater_preferences>\n",
    "\n",
    "## Analysis\n",
    "Critially analyse the <current_judge_prompt> and <rater_preferences> and make recommendations for whether or not \\\n",
    "the <current_judge_prompt> needs to be updated. You are not required to make edits to the <current_judge_prompt> \\\n",
    "if you fell it is already aligned with the <rater_preferences>.\n",
    "\n",
    "## Output\n",
    "Based on your analysis, please output a full updated judge prompt below.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class JudgePromptAnalysis(BaseModel):\n",
    "    reasoning: str = Field(description=\"A detailed explanation of your analysis of the <current_judge_prompt>, \\\n",
    "<rater_preferences> and what edits, if any, are needed to align the <current_judge_prompt> with the rater's preferences.\")\n",
    "    updated_judge_prompt: str = Field(description=\"The updated judge prompt based on the analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "judge_prompt_analyser_prompt.format(current_judge_prompt=seed_judge_prompt, ranking_patterns=ranking_patterns)\n",
    "# judge_prompt_analyser_prompt+=judge_prompt_postfix\n",
    "\n",
    "ANALYZER_MODEL = \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\" \"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\",\n",
    "\n",
    "@weave.op\n",
    "async def run_judge_prompt_analyser():\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=ANALYZER_SYSTEM_INSTRUCTION,\n",
    "        prompt=judge_prompt_analyser_prompt,\n",
    "        model_name=ANALYZER_MODEL,\n",
    "        temperature=1.0,\n",
    "        response_model=JudgePromptAnalysis,\n",
    "        initial_audio_parts_prompt=\"\\n\\nvoice_1:\\n\",\n",
    "        audio_parts_prompt_divider=\"\\n\\nvoice_{input_order}:\\n\",\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Reasoning:**\n",
      "The current judge prompt lacks specific guidance on how to evaluate voice samples based on the identified rater preferences. The rater prioritizes naturalness, clarity, and expressiveness, and penalizes robotic or monotone voices, as well as background noise and audio artifacts. The prompt should be updated to explicitly incorporate these criteria to better align with the rater's judgment. Specifically, I will add instructions to explicitly prioritize samples with natural prosody and intonation, clear articulation, and expressive delivery. Also, I will add instructions to penalize samples with robotic or monotone delivery, background noise, and any audio artifacts (clipping, distortion, etc).\n",
      "\n",
      "**Updated Judge Prompt:**\n",
      "You are a highly skilled AI assistant that evaluates the quality of generated voice samples. Your evaluation should focus on the following criteria, with an emphasis on naturalness, clarity, and expressiveness:\n",
      "\n",
      "1.  Naturalness: Assess how human-like the voice sounds. Prioritize samples with natural prosody and intonation, avoiding robotic or artificial qualities.\n",
      "2.  Clarity: Evaluate the clarity of the speech. The words should be easily understandable, with clear articulation and minimal ambiguity.\n",
      "3.  Expressiveness: Consider the degree to which the voice conveys emotion and engages the listener. Prioritize samples with expressive delivery that suits the content.\n",
      "4.  Absence of Artifacts: Penalize samples with background noise, audio artifacts (clipping, distortion, etc), or other distracting elements.\n",
      "\n",
      "Specifically, penalize samples that exhibit the following:\n",
      "\n",
      "*   Robotic or monotone delivery\n",
      "*   Unclear articulation or muffled speech\n",
      "*   Lack of emotional expressiveness\n",
      "*   Background noise or audio artifacts\n",
      "\n",
      "Provide a comprehensive rating and detailed justification for your assessment.\n"
     ]
    }
   ],
   "source": [
    "analyzer_result = await run_judge_prompt_analyser()\n",
    "\n",
    "print(f\"**Reasoning:**\\n{analyzer_result.reasoning}\\n\")\n",
    "print(f\"**Updated Judge Prompt:**\\n{analyzer_result.updated_judge_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the updated Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_samples_to_eval = speech_samples[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The primary goal is to rank voices based on their suitability as a '\n",
      " \"'frustrated gamer' along with other vocal qualities. \\n\"\n",
      " '\\n'\n",
      " 'Voice 1: Exhibits excellent clarity and naturalness. The pitch is slightly '\n",
      " \"higher and has a strained quality that perfectly matches the 'frustrated \"\n",
      " \"gamer' persona. Intonation effectively conveys rising frustration. Pacing is \"\n",
      " 'good, and energy is high and engaging. The timbre supports the emotional '\n",
      " 'state. Highly suitable for the context.\\n'\n",
      " '\\n'\n",
      " 'Voice 2: Clear and natural, but the pitch is lower and the delivery is '\n",
      " 'calmer, sounding more like general annoyance or disappointment rather than '\n",
      " 'acute gamer frustration. While expressive, the energy level is lower '\n",
      " 'compared to the others for this specific context. Timbre is pleasant but '\n",
      " \"doesn't convey the same level of agitation. Less suitable for a 'frustrated \"\n",
      " \"gamer' but good for a generally annoyed tone.\\n\"\n",
      " '\\n'\n",
      " 'Voice 3: Excellent clarity and naturalness. The pitch is appropriate, and '\n",
      " \"the intonation strongly conveys frustration, especially in the line 'GTA 6 \"\n",
      " \"was supposed to be out by now'. Energy is high and engaging. Timbre is clear \"\n",
      " 'and pleasant. Very suitable for the context, almost on par with Voice 1.\\n'\n",
      " '\\n'\n",
      " 'Ranking Rationale:\\n'\n",
      " 'Voice 1 is ranked first because its slightly higher, strained pitch and '\n",
      " 'overall delivery most convincingly portray an exasperated, frustrated gamer. '\n",
      " 'The build-up of frustration feels very authentic to the scenario.\\n'\n",
      " \"Voice 3 is a very close second. It's highly expressive and energetic, \"\n",
      " \"clearly conveying frustration. It's an excellent fit for the context, \"\n",
      " 'perhaps representing a slightly different style of gamer frustration but '\n",
      " 'equally valid.\\n'\n",
      " \"Voice 2 is ranked third because while it's a good quality voice, its lower \"\n",
      " 'energy and calmer, more measured delivery of frustration make it less '\n",
      " \"fitting for the specific 'frustrated gamer' context compared to the other \"\n",
      " 'two. It sounds more like someone who is generally annoyed or disappointed.')\n",
      "['voice_1', 'voice_3', 'voice_2']\n",
      "\n",
      "weary_georgia_mutter_20250526_2033 c8ab9b2d\n",
      "hushed_georgia_rush_20250526_2033 adcc9dc3\n",
      "surprised_french_veteran_20250526_2033 39de6b56\n",
      "\n",
      "Human Ranking:\n",
      "(\"Voice 1: {'rank': 1, 'id': 'weary_georgia_mutter_20250526_2033', \"\n",
      " \"'original_input_order': 2, 'short_hash': 'c8ab9b2d'}\")\n",
      "(\"Voice 2: {'rank': 2, 'id': 'hushed_georgia_rush_20250526_2033', \"\n",
      " \"'original_input_order': 3, 'short_hash': 'adcc9dc3'}\")\n",
      "(\"Voice 3: {'rank': 3, 'id': 'surprised_french_veteran_20250526_2033', \"\n",
      " \"'original_input_order': 1, 'short_hash': '39de6b56'}\")\n"
     ]
    }
   ],
   "source": [
    "speech_samples_to_eval = speech_samples[-10]\n",
    "\n",
    "BASIC_JUDGE_SYSTEM_INSTRUCTION = \"Assess the generated voices provided\"\n",
    "judge_prompt = f\"\"\"Based on the following criteria, your task is to rank the voices proivded:\\\n",
    "\n",
    "{}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@weave.op\n",
    "async def run_speech_judge():\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=BASIC_JUDGE_SYSTEM_INSTRUCTION,\n",
    "        prompt=judge_prompt,\n",
    "        model_name=\"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\",\n",
    "        temperature=0.1,\n",
    "        response_model=JudgeRanking,\n",
    "        audio_data=[\n",
    "            samples_with_rankings[0][\"audio_bytes\"],\n",
    "            samples_with_rankings[1][\"audio_bytes\"],\n",
    "            samples_with_rankings[2][\"audio_bytes\"],\n",
    "        ],\n",
    "        initial_audio_parts_prompt=\"\\n\\nvoice_1:\\n\",\n",
    "        audio_parts_prompt_divider=\"\\n\\nvoice_{input_order}:\\n\",\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run example\n",
    "# result = asyncio.run(example())\n",
    "result = await run_speech_judge()\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()\n",
    "\n",
    "# for s in samples_with_rankings:\n",
    "#     print(s.get(\"id\"), s.get(\"short_hash\"))\n",
    "# print()\n",
    "\n",
    "print(\"Human Ranking:\")\n",
    "for i in range(3):\n",
    "    pprint(f\"Voice {i + 1}: {final_rankings[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
