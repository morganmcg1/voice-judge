{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases is Great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/morganmcg1/voice-judge.git && cp -r voice-judge/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r pyproject.toml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "import weave\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from audio_utils import AudioRanker, wave_read_to_wav_bytes\n",
    "from judge import (\n",
    "    JudgeRanking,\n",
    "    run_speech_llm,\n",
    "    update_pairwise_comparison_history,\n",
    "    # update_pairwise_comparison_history_from_feedback\n",
    ")\n",
    "from preference_learner import PreferenceLearner, USER_CONTEXT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: morgan.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-voice-ai/voice-judge/weave\n"
     ]
    }
   ],
   "source": [
    "weave_client = weave.init(\"wandb-voice-ai/voice-judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading speech samples from Weave...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b1-92c5-7943-a36b-533e40d5a4e8\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b1-e0fc-72d1-a2c5-924020c65cf4\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b4-2177-7bc1-8ad8-9f7af54bccfb\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b6-a97d-7c83-bf52-0e69c3f7fec8\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b7-1463-79e2-804c-0775d5fb49d3\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-voice-ai/voice-judge/r/call/019712b9-608f-7bc0-a0a7-46061852d832\n"
     ]
    }
   ],
   "source": [
    "# SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio:v1\"\n",
    "SPEECH_AUDIO_DATASET_URI = \"weave:///wandb-voice-ai/voice-judge/object/generated_speech_audio_test\"\n",
    "N_SAMPLES_TO_RANK = 3\n",
    "PREFERENCE_LEARNER_MODEL =  \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\"\n",
    "JUDGE_MODEL =  \"gemini-2.0-flash\" # \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "print(\"Downloading speech samples from Weave...\")\n",
    "ds_ref = weave.ref(SPEECH_AUDIO_DATASET_URI).get()\n",
    "speech_samples = list(ds_ref.rows)\n",
    "\n",
    "samples_to_rank = {}\n",
    "for i in range(N_SAMPLES_TO_RANK):\n",
    "    samples_to_rank[speech_samples[i][\"voice_instructions_id\"]] = {\n",
    "        \"audio\": speech_samples[i][\"audio\"],\n",
    "        \"audio_bytes\": wave_read_to_wav_bytes(speech_samples[i][\"audio\"]),\n",
    "        \"instructions\": speech_samples[i][\"voice_instructions\"],\n",
    "        \"short_hash\": None,\n",
    "        \"pairwise_comparison_history\": {},\n",
    "    }\n",
    "\n",
    "ids_to_rank = list(samples_to_rank.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our samples for ranking\n",
    "@weave.op\n",
    "def rank_audio(sample_1: dict, sample_2: dict) -> None:\n",
    "    pass\n",
    "\n",
    "sample_1_id = ids_to_rank[0]\n",
    "sample_2_id = ids_to_rank[1]\n",
    "\n",
    "sample_1 = {\"id\": sample_1_id, **samples_to_rank[sample_1_id]}\n",
    "sample_2 = {\"id\": sample_2_id, **samples_to_rank[sample_2_id]}\n",
    "\n",
    "_, target_call = rank_audio.call(sample_1, sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-voice-ai/voice-judge/weave/objects/AudioRanker/versions/ym5a2rKViLvZtIjKh5VMKYzRxpZOmGhGDTLgSi8hDN8\n"
     ]
    }
   ],
   "source": [
    "# Init Ranker\n",
    "\n",
    "ranker = AudioRanker(\n",
    "    [\n",
    "        {\n",
    "            \"id\": sample_1_id,\n",
    "            \"audio\": samples_to_rank[sample_1_id][\"audio\"],\n",
    "            \"original_input_order\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"id\": sample_2_id,\n",
    "            # \"audio\": samples_to_rank[sample_2_id][\"audio_bytes\"],\n",
    "            \"audio\": samples_to_rank[sample_2_id][\"audio\"],\n",
    "            \"original_input_order\": 2,\n",
    "        },\n",
    "    ],\n",
    "    weave_client=weave_client,\n",
    "    target_call=target_call,\n",
    "    image_path=\"assets/gta-vi_guy_in_front_of_truck_cropped.jpg\",\n",
    "    mode=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BinaryVoiceRank applied successfully to weave call. Result: ApplyScorerSuccess(result={'preferred_sample_id': 'weary_georgia_mutter_20250526_2033', 'sample_one_preferred': False, 'sample_two_preferred': True, 'ranking_timestamp': '2025-05-27_18-02'}, score_call=Call(_op_name=<Future at 0x11b180a10 state=running>, trace_id='019712b1-e0fc-72d1-a2c5-9237d50b0985', project_id='wandb-voice-ai/voice-judge', parent_id=None, inputs={'self': ObjectRef(entity='wandb-voice-ai', project='voice-judge', name='BinaryVoiceRank', _digest=<Future at 0x11aee9390 state=pending>, _extra=()), 'output': None}, id='019712b1-e0fc-72d1-a2c5-924020c65cf4', output={'preferred_sample_id': 'weary_georgia_mutter_20250526_2033', 'sample_one_preferred': False, 'sample_two_preferred': True, 'ranking_timestamp': '2025-05-27_18-02'}, exception=None, summary={'status_counts': {<TraceStatus.SUCCESS: 'success'>: 1, <TraceStatus.ERROR: 'error'>: 0}}, _display_name=None, attributes=AttributesDict({'weave': {'python': {'type': 'function'}, 'client_version': '0.51.48', 'source': 'python-sdk', 'sys_version': '3.11.11 (main, Mar 17 2025, 10:36:47) [Clang 16.0.0 (clang-1600.0.26.6)]', 'os_name': 'Darwin', 'os_version': 'Darwin Kernel Version 24.5.0: Tue Apr 22 19:48:46 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8103', 'os_release': '24.5.0'}}), started_at=None, ended_at=datetime.datetime(2025, 5, 27, 17, 2, 16, 829243, tzinfo=datetime.timezone.utc), deleted_at=None, _children=[], _feedback=None))\n"
     ]
    }
   ],
   "source": [
    "interface = ranker.create_gradio_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do ranking\n",
    "# ranker.display_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "updated_target_call = weave_client.get_call(target_call.id)\n",
    "updated_target_call.feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings retried from Weave\n"
     ]
    }
   ],
   "source": [
    "for f in updated_target_call.feedback.feedbacks.items:\n",
    "    if \"output\" in f.payload:\n",
    "        ranking = f.payload[\"output\"]\n",
    "        print(\"Rankings retried from Weave\")\n",
    "        break\n",
    "\n",
    "rank1 = samples_to_rank[ranking[\"preferred_sample_id\"]]\n",
    "\n",
    "if ranking[\"preferred_sample_id\"] == sample_1_id:\n",
    "    rank2 = samples_to_rank[sample_2_id]\n",
    "    rank2_id = sample_2_id\n",
    "else:\n",
    "    rank2 = samples_to_rank[sample_1_id]\n",
    "    rank2_id = sample_1_id\n",
    "\n",
    "res = [\n",
    "    {\n",
    "        \"rank\" : 1,\n",
    "        \"id\" : ranking[\"preferred_sample_id\"],\n",
    "        \"original_input_order\" : 1 if ranking[\"preferred_sample_id\"] == sample_1_id else 2\n",
    "    },\n",
    "    {\n",
    "        \"rank\" : 2,\n",
    "        \"id\" : rank2_id,\n",
    "        \"original_input_order\" : 1 if rank2_id== sample_1_id else 2\n",
    "    }\n",
    "]\n",
    "\n",
    "final_rankings = {\"rankings\": res, \n",
    "                  \"completed_at\": ranking[\"ranking_timestamp\"],\n",
    "                  \"preferred_id\": ranking[\"preferred_sample_id\"],\n",
    "                  \"rejected_id\": rank2_id\n",
    "                  }\n",
    "final_rankings\n",
    "\n",
    "samples_to_rank = update_pairwise_comparison_history(samples_to_rank, final_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('surprised_french_veteran_20250526_2033',\n",
       " {'2025-05-27_18-02': {'competitor_id': 'weary_georgia_mutter_20250526_2033',\n",
       "   'sample_rank_in_this_pair': 2}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1_id, samples_to_rank[sample_1_id][\"pairwise_comparison_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strong': [], 'emerging': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PreferenceLearner()\n",
    "learner.patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating comparisons...\n",
      "Running pattern update...\n",
      "\n",
      "\n",
      "Pattern update result:\n",
      "('reasoning: In the provided example, the preferred voice exhibits a slight '\n",
      " 'increase in pitch and more varied intonation, making it sound more '\n",
      " 'expressive and engaging. The rejected voice sounds flatter and less '\n",
      " 'enthusiastic. This suggests that higher pitch and varied intonation may be '\n",
      " 'preferred. Since there are no existing strong or emerging patterns, these '\n",
      " 'are considered new.')\n",
      "'strong: []'\n",
      "\"emerging: ['Higher pitch is preferred.', 'Varied intonation is preferred.']\"\n"
     ]
    }
   ],
   "source": [
    "await learner.update(final_rankings, samples_to_rank)\n",
    "ranking_patterns = learner.patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_JUDGE_SYSTEM_INSTRUCTION = f\"\"\"Assess the generated voices provided. The task is to assess the appropriateness \\\n",
    "of the audio voice sample for the given task. If multiple voice samples are provided, rank them in order of preference.\n",
    "\n",
    "Here is some context about the task:\n",
    "\n",
    "{USER_CONTEXT_PROMPT}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed_judge_prompt = f\"\"\"Based on the following criteria, the task is to assess the appropriateness \\\n",
    "of the audio voice sample for a video game character in his late 50's. He should sound like a man in his late 50's \\\n",
    "and be a little on the wild side.\n",
    "\n",
    "## Voice characteristics to consider\n",
    "Consider the following aspects of the voice:\n",
    "- style\n",
    "- tone\n",
    "- accent\n",
    "- speed\n",
    "- volume\n",
    "- pitch\n",
    "- intonation\n",
    "and more\n",
    "\"\"\"\n",
    "\n",
    "judge_prompt_postfix = f\"\"\"## Assessment\n",
    "- If a single voice sample is provided, assess it according to the above criteria and return a bool of whether \\\n",
    "it is appropriate.\n",
    "- If multiple voice samples are provided, rank them in order of preference according to the above criteria.\n",
    "\"\"\"\n",
    "\n",
    "# ---------Analyzer Prompts ---------\n",
    "\n",
    "ANALYZER_SYSTEM_INSTRUCTION = f\"\"\"The task is to help align a LLLM Judge towards user preferences. This will \\\n",
    "be acheived by analysing the current prompt as well as new preference data derived from the user, followed by \\\n",
    "making recommendations for how to update the prompt to align more with the user's preferences.\"\"\"\n",
    "\n",
    "judge_prompt_analyser_prompt = \"\"\"The task is to optimze a given LLM judge prompt to align more with a human \\\n",
    "rater's preferences.\n",
    "\n",
    "## Current prompt\n",
    "Below is the current judge prompt: \n",
    "\n",
    "<current_judge_prompt>\n",
    "{current_judge_prompt}\n",
    "</current_judge_prompt>\n",
    "\n",
    "## Rater preferences\n",
    "Below are patterns that have been observed from a human rater's preferences when doing pairwise comparisons of \\\n",
    "voice samples.\n",
    "\n",
    "<rater_preferences>\n",
    "{ranking_patterns}\n",
    "</rater_preferences>\n",
    "\n",
    "## Analysis\n",
    "Critially analyse the <current_judge_prompt> and <rater_preferences> and make recommendations for whether or not \\\n",
    "the <current_judge_prompt> needs to be updated. You are not required to make edits to the <current_judge_prompt> \\\n",
    "if you fell it is already aligned with the <rater_preferences>.\n",
    "\n",
    "## Output\n",
    "Based on your analysis, please output a full updated judge prompt below. Do not include any placeholder text \\\n",
    "for the input variables. Just focus on describing what makes a good and bad voice sample.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class JudgePromptAnalysis(BaseModel):\n",
    "    reasoning: str = Field(description=\"A detailed explanation of your analysis of the <current_judge_prompt>, \\\n",
    "<rater_preferences> and what edits, if any, are needed to align the <current_judge_prompt> with the rater's preferences.\")\n",
    "    updated_judge_prompt: str = Field(description=\"The updated judge prompt based on the analysis, focussed just on the \\\n",
    "characteristics of a good and bad voice sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the seed judge prompt plus preference analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def run_judge_prompt_analyser(judge_prompt_analyser_prompt: str, analyzer_model: str) -> JudgePromptAnalysis:\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=ANALYZER_SYSTEM_INSTRUCTION,\n",
    "        prompt=judge_prompt_analyser_prompt,\n",
    "        model_name=analyzer_model,\n",
    "        temperature=1.0,\n",
    "        response_model=JudgePromptAnalysis,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_analyser_prompt.format(current_judge_prompt=seed_judge_prompt, ranking_patterns=ranking_patterns)\n",
    "ANALYZER_MODEL = \"gemini-2.0-flash\" #\"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "analyzer_result = await run_judge_prompt_analyser(\n",
    "    judge_prompt_analyser_prompt=judge_prompt_analyser_prompt,\n",
    "    analyzer_model=ANALYZER_MODEL\n",
    ")\n",
    "\n",
    "pprint(f\"**Reasoning:**\\n{analyzer_result.reasoning}\\n\")\n",
    "pprint(f\"**Updated Judge Prompt:**\\n{analyzer_result.updated_judge_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the updated Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_samples_to_eval = speech_samples[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The primary goal is to rank voices based on their suitability as a '\n",
      " \"'frustrated gamer' along with other vocal qualities. \\n\"\n",
      " '\\n'\n",
      " 'Voice 1: Exhibits excellent clarity and naturalness. The pitch is slightly '\n",
      " \"higher and has a strained quality that perfectly matches the 'frustrated \"\n",
      " \"gamer' persona. Intonation effectively conveys rising frustration. Pacing is \"\n",
      " 'good, and energy is high and engaging. The timbre supports the emotional '\n",
      " 'state. Highly suitable for the context.\\n'\n",
      " '\\n'\n",
      " 'Voice 2: Clear and natural, but the pitch is lower and the delivery is '\n",
      " 'calmer, sounding more like general annoyance or disappointment rather than '\n",
      " 'acute gamer frustration. While expressive, the energy level is lower '\n",
      " 'compared to the others for this specific context. Timbre is pleasant but '\n",
      " \"doesn't convey the same level of agitation. Less suitable for a 'frustrated \"\n",
      " \"gamer' but good for a generally annoyed tone.\\n\"\n",
      " '\\n'\n",
      " 'Voice 3: Excellent clarity and naturalness. The pitch is appropriate, and '\n",
      " \"the intonation strongly conveys frustration, especially in the line 'GTA 6 \"\n",
      " \"was supposed to be out by now'. Energy is high and engaging. Timbre is clear \"\n",
      " 'and pleasant. Very suitable for the context, almost on par with Voice 1.\\n'\n",
      " '\\n'\n",
      " 'Ranking Rationale:\\n'\n",
      " 'Voice 1 is ranked first because its slightly higher, strained pitch and '\n",
      " 'overall delivery most convincingly portray an exasperated, frustrated gamer. '\n",
      " 'The build-up of frustration feels very authentic to the scenario.\\n'\n",
      " \"Voice 3 is a very close second. It's highly expressive and energetic, \"\n",
      " \"clearly conveying frustration. It's an excellent fit for the context, \"\n",
      " 'perhaps representing a slightly different style of gamer frustration but '\n",
      " 'equally valid.\\n'\n",
      " \"Voice 2 is ranked third because while it's a good quality voice, its lower \"\n",
      " 'energy and calmer, more measured delivery of frustration make it less '\n",
      " \"fitting for the specific 'frustrated gamer' context compared to the other \"\n",
      " 'two. It sounds more like someone who is generally annoyed or disappointed.')\n",
      "['voice_1', 'voice_3', 'voice_2']\n",
      "\n",
      "weary_georgia_mutter_20250526_2033 c8ab9b2d\n",
      "hushed_georgia_rush_20250526_2033 adcc9dc3\n",
      "surprised_french_veteran_20250526_2033 39de6b56\n",
      "\n",
      "Human Ranking:\n",
      "(\"Voice 1: {'rank': 1, 'id': 'weary_georgia_mutter_20250526_2033', \"\n",
      " \"'original_input_order': 2, 'short_hash': 'c8ab9b2d'}\")\n",
      "(\"Voice 2: {'rank': 2, 'id': 'hushed_georgia_rush_20250526_2033', \"\n",
      " \"'original_input_order': 3, 'short_hash': 'adcc9dc3'}\")\n",
      "(\"Voice 3: {'rank': 3, 'id': 'surprised_french_veteran_20250526_2033', \"\n",
      " \"'original_input_order': 1, 'short_hash': '39de6b56'}\")\n"
     ]
    }
   ],
   "source": [
    "speech_samples_to_eval = speech_samples[-10]\n",
    "\n",
    "BASIC_JUDGE_SYSTEM_INSTRUCTION = \"Assess the generated voices provided\"\n",
    "judge_prompt = f\"\"\"Based on the following criteria, your task is to rank the voices proivded:\\\n",
    "\n",
    "{}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@weave.op\n",
    "async def run_speech_judge():\n",
    "    # Example with file path\n",
    "    result = await run_speech_llm(\n",
    "        system_instruction=BASIC_JUDGE_SYSTEM_INSTRUCTION,\n",
    "        prompt=judge_prompt,\n",
    "        model_name=\"gemini-2.5-pro-preview-05-06\",  # \"gemini-2.0-flash\",\n",
    "        temperature=0.1,\n",
    "        response_model=JudgeRanking,\n",
    "        audio_data=[\n",
    "            samples_with_rankings[0][\"audio_bytes\"],\n",
    "            samples_with_rankings[1][\"audio_bytes\"],\n",
    "            samples_with_rankings[2][\"audio_bytes\"],\n",
    "        ],\n",
    "        initial_audio_parts_prompt=\"\\n\\nvoice_1:\\n\",\n",
    "        audio_parts_prompt_divider=\"\\n\\nvoice_{input_order}:\\n\",\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run example\n",
    "# result = asyncio.run(example())\n",
    "result = await run_speech_judge()\n",
    "pprint(result.thinking)\n",
    "pprint(result.ranking)\n",
    "print()\n",
    "\n",
    "# for s in samples_with_rankings:\n",
    "#     print(s.get(\"id\"), s.get(\"short_hash\"))\n",
    "# print()\n",
    "\n",
    "print(\"Human Ranking:\")\n",
    "for i in range(3):\n",
    "    pprint(f\"Voice {i + 1}: {final_rankings[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
